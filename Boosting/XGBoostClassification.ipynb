{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost for multi-classification\n",
    "- Giả sử bài toán có $C$ lớp.\n",
    "- Sử dụng kỹ thuật One hot coding, mỗi out sẽ là 1 vector, có đúng 1 phần tử bằng 1, các phần tử còn lại bằng 0. Phần tử bằng 1 năm ở vị trí tương ứng với class đó, thể hiện rằng điểm dữ liệu đang xét rơi vào class này với xác suất bằng 1.\n",
    "- Xác suất mô hình dự đoán điểm $i$ rơi vào lớp c là: $$\\hat{P}_{ic} =  \\frac{\\exp{\\hat{y}_{ic}}}{\\sum_{j = 1}^C \\exp{\\hat{y}_{ij}}}$$\n",
    "\n",
    "- Hàm mất mát: Sử dụng hàm Cross Entropy để đánh giá sự khác nhau giữa 2 phân phối xác suất:\n",
    "    $$ H(\\mathbf{p}, \\mathbf{q}) = -\\sum_{i = 1}^N p_i \\log(q_i) $$\n",
    "\n",
    "    + Áp dụng vào bài toán: Hàm mất mát tại điểm dữ liệu thứ $i$:\n",
    "    $$\n",
    "    \\begin{align*}\n",
    "    L(y_i, \\hat{y}_i) &= -\\sum_{j=1}^C y_{ij} \\log(\\hat{P}_{ij}) \\\\\n",
    "      &= -\\sum_{j=1}^C y_{ij} \\log \\left( \\frac{\\exp(\\hat{y}_{ij})}{\\sum_{c=1}^C \\exp(\\hat{y}_{ic})} \\right) \\\\\n",
    "      &= -\\sum_{j=1}^C ( y_{ij} \\hat{y}_{ij} - y_{ij} \\log \\sum_{c=1}^C \\exp(\\hat{y}_{ic})) \\\\ \n",
    "      &= -\\sum_{j=1}^C y_{ij} \\hat{y}_{ij} +  ( \\sum_{j=1}^C y_{ij})  \\log \\sum_{c=1}^C \\exp(\\hat{y}_{ic}) \\\\\n",
    "      &= -\\sum_{j=1}^C y_{ij} \\hat{y}_{ij} +  \\log \\sum_{c=1}^C \\exp(\\hat{y}_{ic}) \\ \\ \\ \\ \\ \\ \\text{Do}  \\sum_{j=1}^C y_{ij} = 1 \\\\\n",
    "    \\end{align*}\n",
    "    $$\n",
    "\n",
    "\n",
    "    + Tính gradient:\n",
    "    $$\n",
    "    \\begin{align*}\n",
    "    \\frac{\\partial L}{ \\partial \\hat{y}_{ij}} &= - y_{ij} + \\frac{\\exp{\\hat{y}_{ij}}}{\\sum_{c=1}^C \\exp(\\hat{y}_{ic})}  \\\\\n",
    "                                              &= - y_{ij} + \\hat{P}_{ij}\n",
    "    \\end{align*}\n",
    "    $$\n",
    "\n",
    "    Khi đó gradient:\n",
    "    $$\n",
    "    \\begin{align*}\n",
    "    g_i = \\frac{\\partial L}{ \\partial \\hat{y}_{i}} &= \\left[\\frac{\\partial L}{ \\partial \\hat{y}_{i1}},..., \\frac{\\partial L}{ \\partial \\hat{y}_{iC}} \\right]  \\\\\n",
    "                                             &= \\left[- y_{i1} + \\hat{P}_{i1},..., - y_{ij} + \\hat{P}_{ic} \\right]\n",
    "    \\end{align*}\n",
    "    $$\n",
    "\n",
    "    + Tính Hessian: \n",
    "    $$\n",
    "    \\begin{align*}\n",
    "    \\frac{\\partial^2 L}{ \\partial^2 \\hat{y}_{ij}} \n",
    "    &= \\frac{\\exp{(\\hat{y}_{ij})} \\sum_{c=1}^C \\exp(\\hat{y}_{ic}) - (\\exp{(\\hat{y}_{ij})})^2}{(\\sum_{c=1}^C \\exp(\\hat{y}_{ic}))^2}  \\\\\n",
    "    &= \\frac{\\exp{(\\hat{y}_{ij})}}{\\sum_{c=1}^C \\exp(\\hat{y}_{ic})} \\left(1 - \\frac{\\exp{(\\hat{y}_{ij})}}{\\sum_{c=1}^C \\exp(\\hat{y}_{ic})} \\right) \\\\\n",
    "    &= \\hat{P}_{ij} (1 - \\hat{P}_{ij})\n",
    "    \\end{align*}\n",
    "    $$\n",
    "\n",
    "- Dự đoán: giá trị dự đoán dựa trên lớp có xác suất lớn nhất"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "class TreeBooster:\n",
    "    def __init__(self, X, gradients, hessians, max_depth, min_child_weight, reg_lambda, gamma, idxs=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_child_weight = min_child_weight\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        if idxs is None: \n",
    "            idxs = np.arange(len(gradients))\n",
    "        self.X, self.gradients, self.hessians, self.idxs = X, gradients, hessians, idxs\n",
    "        self.n, self.c = len(idxs), X.shape[1]\n",
    "        self.value_predict = -self.gradients[self.idxs].sum() / (self.hessians[self.idxs].sum() + self.reg_lambda)\n",
    "        self.best_score_so_far = 0.\n",
    "        self.best_feature_type = None\n",
    "        if self.max_depth > 0:\n",
    "            self._maybe_insert_child_nodes()\n",
    "\n",
    "    def _maybe_insert_child_nodes(self):\n",
    "        for i in range(self.c):\n",
    "            self._find_better_split(i)\n",
    "        if self.is_leaf():\n",
    "            return\n",
    "        x = self.X[self.idxs, self.split_feature_idx]\n",
    "        if self.best_feature_type == 'Categorical':\n",
    "            left_idx = self.idxs[x == self.threshold]\n",
    "            right_idx = self.idxs[x != self.threshold]\n",
    "        else:\n",
    "            left_idx = self.idxs[x <= self.threshold]\n",
    "            right_idx = self.idxs[x > self.threshold]\n",
    "        self.left = TreeBooster(self.X, self.gradients, self.hessians, self.max_depth - 1, self.min_child_weight, self.reg_lambda, self.gamma, left_idx)\n",
    "        self.right = TreeBooster(self.X, self.gradients, self.hessians, self.max_depth - 1, self.min_child_weight, self.reg_lambda, self.gamma, right_idx)\n",
    "\n",
    "    def is_leaf(self):\n",
    "        return self.best_score_so_far == 0.\n",
    "\n",
    "    def _find_better_split(self, feature_idx):\n",
    "        x = self.X[self.idxs, feature_idx]\n",
    "        g, h = self.gradients[self.idxs], self.hessians[self.idxs]\n",
    "        sum_g, sum_h = g.sum(), h.sum()\n",
    "        \n",
    "        # Thuộc tính kiểu phân loại\n",
    "        if x.dtype.kind in ['b', 'O']:\n",
    "            #print('o')\n",
    "            unique_elements = np.unique(x)\n",
    "            ids_left = (x == unique_elements[0])\n",
    "            ids_right = ~ids_left\n",
    "            sum_g_left = g[ids_left].sum() \n",
    "            sum_h_left = h[ids_left].sum()\n",
    "            sum_g_right = g[ids_right].sum()\n",
    "            sum_h_right = h[ids_right].sum()\n",
    "\n",
    "            gain = 0.5 * ((sum_g_left**2 / (sum_h_left + self.reg_lambda))\n",
    "                            + (sum_g_right**2 / (sum_h_right + self.reg_lambda))\n",
    "                            - (sum_g**2 / (sum_h + self.reg_lambda))) - self.gamma\n",
    "\n",
    "            if gain > self.best_score_so_far:\n",
    "                self.split_feature_idx = feature_idx\n",
    "                self.best_score_so_far = gain\n",
    "                self.threshold = unique_elements[0]\n",
    "                self.best_feature_type = 'Categorical'\n",
    "        \n",
    "        #Thuộc tính kiểu số\n",
    "        else:\n",
    "            sort_idx = np.argsort(x)\n",
    "            sort_g, sort_h, sort_x = g[sort_idx], h[sort_idx], x[sort_idx]\n",
    "            sum_g_right, sum_h_right = sum_g, sum_h\n",
    "            sum_g_left, sum_h_left = 0., 0.\n",
    "\n",
    "            for i in range(0, self.n - 1):\n",
    "                g_i, h_i, x_i, x_i_next = sort_g[i], sort_h[i], sort_x[i], sort_x[i + 1]\n",
    "                sum_g_left += g_i\n",
    "                sum_h_left += h_i\n",
    "                sum_g_right -= g_i\n",
    "                sum_h_right -= h_i\n",
    "                if sum_h_left < self.min_child_weight or x_i == x_i_next:\n",
    "                    continue\n",
    "                if sum_h_right < self.min_child_weight:\n",
    "                    break\n",
    "\n",
    "                gain = 0.5 * ((sum_g_left**2 / (sum_h_left + self.reg_lambda))\n",
    "                            + (sum_g_right**2 / (sum_h_right + self.reg_lambda))\n",
    "                            - (sum_g**2 / (sum_h + self.reg_lambda))) - self.gamma\n",
    "\n",
    "                if gain > self.best_score_so_far:\n",
    "                    self.split_feature_idx = feature_idx\n",
    "                    self.best_score_so_far = gain\n",
    "                    self.threshold = (x_i + x_i_next) / 2\n",
    "                    self.best_feature_type = 'numeric'\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict_row(row) for row in X])\n",
    "\n",
    "    def _predict_row(self, row):\n",
    "        if self.is_leaf():\n",
    "            return self.value_predict\n",
    "        \n",
    "        child = self.left if row[self.split_feature_idx] <= self.threshold else self.right\n",
    "        return child._predict_row(row)\n",
    "\n",
    "\n",
    "class XGBoostMultiClassifier:\n",
    "    def __init__(self, learning_rate=0.1, max_depth=3, min_child_weight=1, gamma=0, reg_lambda=1, subsample=1.0, num_boost_round=100, random_seed=None):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.min_child_weight = min_child_weight\n",
    "        self.gamma = gamma\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.subsample = subsample\n",
    "        self.num_boost_round = num_boost_round\n",
    "        self.rng = np.random.default_rng(seed=random_seed)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.num_classes = len(np.unique(y))\n",
    "        self.boosters = [[] for _ in range(self.num_classes)]\n",
    "        if isinstance(X, pd.DataFrame): \n",
    "            X = X.values\n",
    "        if isinstance(y, pd.Series): \n",
    "            y = y.values\n",
    "\n",
    "        y_one_hot = np.zeros((len(y), self.num_classes))\n",
    "\n",
    "        for i in range(len(y)):\n",
    "            y_one_hot[i, y[i]] = 1\n",
    "\n",
    "        current_predictions = np.zeros((len(y), self.num_classes))\n",
    "        \n",
    "        for i in range(self.num_boost_round):\n",
    "            for k in range(self.num_classes):\n",
    "                # giảm thiểu tràn số\n",
    "                probs = np.exp(current_predictions - np.max(current_predictions, axis=1, keepdims=True))\n",
    "                probs /= probs.sum(axis=1, keepdims=True)\n",
    "                #print(probs[1].sum())\n",
    "                gradients = probs[:, k] - y_one_hot[:, k]\n",
    "                hessians = probs[:, k] * (1 - probs[:, k])\n",
    "\n",
    "                if self.subsample == 1.0:\n",
    "                    sample_idxs = None\n",
    "                else:\n",
    "                    sample_idxs = self.rng.choice(len(y),\n",
    "                                        size=math.floor(self.subsample * len(y)),\n",
    "                                        replace=False)\n",
    "                    \n",
    "                booster = TreeBooster(X, gradients, hessians, self.max_depth, self.min_child_weight, self.reg_lambda, self.gamma, sample_idxs)\n",
    "                current_predictions[:, k] += self.learning_rate * booster.predict(X)\n",
    "                self.boosters[k].append(booster)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        log_odds = np.array([np.sum([booster.predict(X) for booster in class_boosters], axis=0) for class_boosters in self.boosters])\n",
    "        log_odds = log_odds.T \n",
    "        probabilities = np.exp(log_odds) / np.exp(log_odds).sum(axis=1, keepdims=True)\n",
    "        return probabilities\n",
    "\n",
    "    def predict(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        probabilities = self.predict_proba(X)\n",
    "        return np.argmax(probabilities, axis=1)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('car_evaluation.csv')\n",
    "\n",
    "X = data.iloc[:, :-1]\n",
    "y = data.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import  train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "label = LabelEncoder()\n",
    "y_train_1 = label.fit_transform(y_train)\n",
    "y_test_1 = label.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9682080924855492"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_xgb = XGBoostMultiClassifier()\n",
    "my_xgb.fit(X_train, y_train_1)\n",
    "pred = my_xgb.predict(X_test)\n",
    "accuracy_score(pred, y_test_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sử dụng thư viện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9739884393063584"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "xg_sklearn = xgb.XGBClassifier()\n",
    "xg_sklearn.fit(X_train, y_train_1)\n",
    "pred = xg_sklearn.predict(X_test)\n",
    "accuracy_score(y_test_1, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
